{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5CIQelq6wsny"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl tiktoken cohere openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kcjJqU8O0qFN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig,get_peft_model , prepare_model_for_int8_training\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer,TrainingArguments\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Lv2qGsSX0_g0"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    train_ds = load_dataset(\"tatsu-lab/alpaca\",split=\"train\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-base\",trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"Salesforce/xgen-7b-8k-base\",\n",
        "        load_in_4bit = True,\n",
        "        torch_dtype = torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "    peft_config = LoraConfig(r=16,lora_alpha = 32,lora_dropout = 0.05,bias =\"none\",task_type=\"CASUAL\")\n",
        "    model = get_peft_model(model,peft_config)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = \"xgen-7b-tuned-alpaca\",\n",
        "        per_device_train_batch_size=4,\n",
        "        optim=\"adamw_torch\",\n",
        "        logging_steps = 100,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = True,\n",
        "        warmup_ratio = 0.1,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        num_train_epochs = 1,\n",
        "        save_strategy = \"epoch\",\n",
        "        push_to_hub=True\n",
        "    )\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        train_dataset = train_ds,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = 1024,\n",
        "        tokenizer = tokenizer,\n",
        "        args = training_args,\n",
        "        packing = True,\n",
        "        peft_config = peft_config\n",
        "      )\n",
        "    trainer.train()\n",
        "    trainer.push_to_hub()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "sROvUWy47jxn",
        "outputId": "383db569-2c63-401e-fbfd-72a3a8f8a9c6"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzi0oy0f7e2h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
